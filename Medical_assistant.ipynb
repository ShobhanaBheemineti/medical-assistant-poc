{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing and Importing Necessary Libraries and Dependencies\n"
      ],
      "metadata": {
        "id": "t0MV44iUwxv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade --force-reinstall \\\n",
        "#   \"numpy>=1.26.0,<2.2.0\" \\\n",
        "#   \"pandas>=2.0.0,<2.2.0\" \\\n",
        "#   \"scikit-learn<1.7\" \\\n",
        "#   \"tensorflow==2.19.0\"\n",
        "\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q"
      ],
      "metadata": {
        "id": "RCSIpymrw2kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries for processing dataframes,text\n",
        "import json,os\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "\n",
        "#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "#Libraries for downloading and loading the llm\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "6PEaY37hxMaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example on how embedding can be split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "sample_text = \"Medical content snippet here...\"\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
        "chunks = splitter.split_text(sample_text)\n",
        "print(chunks)"
      ],
      "metadata": {
        "id": "mogSJRCG0PlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model details\n",
        "model_name_or_path = \"\" # Hugging Face repo ID\n",
        "model_basename = \"\" # File name in the repo\n",
        "\n",
        "# Download from Hugging Face Hub\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ],
      "metadata": {
        "id": "L9-f1X4exq3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2300,       # context window size\n",
        "    n_gpu_layers=38,  # number of layers to run on GPU\n",
        "    n_batch=512       # batch size for inference\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu8L_W9yVShz",
        "outputId": "c296780b-82e2-408c-87c6-779fc3c367af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def response(query,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
        "    model_output = llm(\n",
        "      prompt=query,\n",
        "      max_tokens=max_tokens,\n",
        "      temperature=temperature,\n",
        "      top_p=top_p,\n",
        "      top_k=top_k\n",
        "    )\n",
        "\n",
        "    return model_output['choices'][0]['text']"
      ],
      "metadata": {
        "id": "uwSmnvrUyLCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Question Answering – Baseline (No System Prompt)\n",
        "\n",
        "**Purpose:**  \n",
        "Establish a baseline for Large Language Model (LLM) responses without predefined role, tone, or style instructions.  \n",
        "This allows comparison against later runs with structured **system prompts** to evaluate improvements in accuracy, tone, and consistency.\n",
        "\n",
        "**Approach:**  \n",
        "- Directly pass user queries to the `response()` function without adding a system-level instruction.  \n",
        "- Let the LLM interpret the intent and produce answers based solely on the raw question.  "
      ],
      "metadata": {
        "id": "z25mulCSzhV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "response1 = response(query1)\n",
        "print(response1)"
      ],
      "metadata": {
        "id": "yfxgpPH5yl0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system prompt for the LLM\n",
        "system_prompt = (\n",
        "    \"You are a knowledgeable and precise medical assistant. \"\n",
        "    \"Answer each question accurately based only on reliable medical sources. \"\n",
        "    \"Provide step-by-step reasoning where needed, and list treatments or protocols clearly. \"\n",
        "    \"If unsure, state that more expert consultation is required.\"\n",
        ")"
      ],
      "metadata": {
        "id": "aK8HWSWQyfs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose:\n",
        "Demonstrate how a carefully crafted system prompt can guide a Large Language Model (LLM) to deliver concise, medically accurate, and structured answers for diverse healthcare-related queries."
      ],
      "metadata": {
        "id": "-1Ietn0bzq4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = system_prompt + \"\\n\" + \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "response(user_input)"
      ],
      "metadata": {
        "id": "y96IbBf-yksn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context Truncation Prevents Model Overload\n",
        "Dynamically truncating retrieved context to fit within the LLM’s token limit (2300 tokens) avoids runtime errors and optimizes the balance between context depth and model performance."
      ],
      "metadata": {
        "id": "3GcN2fZwzGaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Engineering Enhances Answer Quality\n",
        "Designed system and user prompts that incorporate retrieved context and clarify the assistant’s role improve answer relevance, groundedness, and reduce hallucinations."
      ],
      "metadata": {
        "id": "UPfSKaq8zJKP"
      }
    }
  ]
}